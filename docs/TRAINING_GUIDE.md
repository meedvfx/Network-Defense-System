# üß™ Guide d'Entra√Ænement des Mod√®les ‚Äî NDS

Ce guide d√©taille la production des artefacts IA n√©cessaires au fonctionnement du *Network Defense System*. L'entra√Ænement est effectu√© **hors de l'application** (Google Colab, Kaggle, Jupyter local avec GPU) et les fichiers r√©sultants sont d√©pos√©s dans `ai/artifacts/`.

> ‚ö†Ô∏è **Le backend NDS est en mode inf√©rence uniquement.** Il charge les mod√®les pr√©-entra√Æn√©s au d√©marrage via `ModelLoader.load_all()` et les v√©rifie via `ArtifactPaths.all_exist()`.

---

## üì¶ Artefacts √† Produire (6 fichiers)

| # | Fichier | Format | Utilis√© par | R√¥le |
|---|---------|--------|-------------|------|
| 1 | `model_supervised.keras` | Keras SavedModel | `ModelLoader` ‚Üí `SupervisedPredictor` | Classifieur multi-classe (MLP) ‚Äî pr√©dit le type d'attaque |
| 2 | `model_unsupervised.keras` | Keras SavedModel | `ModelLoader` ‚Üí `UnsupervisedPredictor` | Auto-Encodeur ‚Äî d√©tecte les anomalies par erreur de reconstruction |
| 3 | `scaler.pkl` | Joblib | `FeaturePipeline.transform()` | `StandardScaler` fitt√© ‚Äî normalise les features |
| 4 | `encoder.pkl` | Joblib | `FeaturePipeline.decode_label()` | `LabelEncoder` ‚Äî convertit index ‚Üî nom d'attaque |
| 5 | `feature_selector.pkl` | Joblib | `FeaturePipeline.transform()` | `SelectKBest` ‚Äî r√©duit la dimensionnalit√© |
| 6 | `threshold_stats.pkl` | Joblib | `UnsupervisedPredictor` | Dict `{mean, std, threshold}` ‚Äî seuil anomalie Œº+3œÉ |

**Destination** : `ai/artifacts/` (v√©rifi√© par `GET /api/models/status`)

---

## 1. Entra√Ænement du Mod√®le Supervis√©

### Dataset Recommand√©
- **CIC-IDS-2017** ou **CIC-IDS-2018** (CSV, ~2.8M lignes)
- Colonnes : ~78 features num√©riques + colonne `Label` (BENIGN, DDoS, PortScan, etc.)

### Script Complet

```python
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
import tensorflow as tf

# ‚îÄ‚îÄ‚îÄ 1. Chargement et nettoyage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
df = pd.read_csv("CIC-IDS2017.csv")
df.columns = df.columns.str.strip()
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(inplace=True)

X = df.drop(columns=["Label"]).values
y = df["Label"].values

# ‚îÄ‚îÄ‚îÄ 2. Encodage des labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)
num_classes = len(encoder.classes_)
print(f"Classes ({num_classes}): {list(encoder.classes_)}")
joblib.dump(encoder, "encoder.pkl")  # üíæ Artefact 4/6

# ‚îÄ‚îÄ‚îÄ 3. S√©lection de Features ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
selector = SelectKBest(f_classif, k=min(50, X.shape[1]))
X_selected = selector.fit_transform(X, y_encoded)
print(f"Features: {X.shape[1]} ‚Üí {X_selected.shape[1]}")
joblib.dump(selector, "feature_selector.pkl")  # üíæ Artefact 5/6

# ‚îÄ‚îÄ‚îÄ 4. Normalisation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚ö†Ô∏è CRITIQUE : Le scaler DOIT √™tre fitt√© APR√àS le feature_selector
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)
joblib.dump(scaler, "scaler.pkl")  # üíæ Artefact 3/6

# ‚îÄ‚îÄ‚îÄ 5. Split + R√©√©quilibrage (SMOTE) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

# ‚îÄ‚îÄ‚îÄ 6. Architecture MLP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(X_train_bal.shape[1],)),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ‚îÄ‚îÄ‚îÄ 7. Entra√Ænement ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
model.fit(
    X_train_bal, y_train_bal,
    epochs=30, batch_size=256,
    validation_data=(X_test, y_test),
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(patience=3),
    ]
)

# ‚îÄ‚îÄ‚îÄ 8. √âvaluation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
y_pred = model.predict(X_test).argmax(axis=1)
print(classification_report(y_test, y_pred, target_names=encoder.classes_))

# ‚îÄ‚îÄ‚îÄ 9. Export ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
model.save("model_supervised.keras")  # üíæ Artefact 1/6
print("‚úì model_supervised.keras sauvegard√©")
```

---

## 2. Entra√Ænement du Mod√®le Non-Supervis√© (Auto-Encodeur)

### Principe

L'auto-encodeur est entra√Æn√© **exclusivement sur le trafic BENIGN**. En production, les attaques produiront une erreur de reconstruction (MSE) √©lev√©e, d√©passant le seuil Œº+3œÉ calcul√© ici.

### Script Complet

```python
import pandas as pd
import numpy as np
import joblib
import tensorflow as tf

# ‚îÄ‚îÄ‚îÄ 1. Charger les m√™mes preprocesseurs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
scaler = joblib.load("scaler.pkl")
selector = joblib.load("feature_selector.pkl")

# ‚îÄ‚îÄ‚îÄ 2. Extraire uniquement le trafic BENIGN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
df = pd.read_csv("CIC-IDS2017.csv")
df.columns = df.columns.str.strip()
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(inplace=True)

df_benign = df[df["Label"] == "BENIGN"]
X_benign = df_benign.drop(columns=["Label"]).values

# ‚ö†Ô∏è M√äME pipeline que le supervis√© (selector ‚Üí scaler)
X_selected = selector.transform(X_benign)
X_scaled = scaler.transform(X_selected)

n_features = X_scaled.shape[1]
print(f"Samples BENIGN: {X_scaled.shape[0]}, Features: {n_features}")

# ‚îÄ‚îÄ‚îÄ 3. Split Train/Val ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
split = int(0.8 * len(X_scaled))
X_train, X_val = X_scaled[:split], X_scaled[split:]

# ‚îÄ‚îÄ‚îÄ 4. Architecture Encoder-Decoder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
encoding_dim = n_features // 4  # Couche goulot (bottleneck)

autoencoder = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(n_features,)),
    # Encoder
    tf.keras.layers.Dense(n_features // 2, activation='relu'),
    tf.keras.layers.Dense(encoding_dim, activation='relu'),
    # Decoder
    tf.keras.layers.Dense(n_features // 2, activation='relu'),
    tf.keras.layers.Dense(n_features, activation='sigmoid')
])

autoencoder.compile(optimizer='adam', loss='mse')

# ‚îÄ‚îÄ‚îÄ 5. Entra√Ænement (input = output) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
autoencoder.fit(
    X_train, X_train,  # L'AE apprend √† reproduire le trafic normal
    epochs=50, batch_size=256,
    validation_data=(X_val, X_val),
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
    ]
)

# ‚îÄ‚îÄ‚îÄ 6. Calcul du seuil d'anomalie ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
reconstructed = autoencoder.predict(X_val)
mse = np.mean(np.square(X_val - reconstructed), axis=1)

threshold_stats = {
    "mean": float(np.mean(mse)),
    "std": float(np.std(mse)),
    "threshold": float(np.mean(mse) + 3 * np.std(mse)),  # Œº + 3œÉ
}
print(f"Seuil anomalie : Œº={threshold_stats['mean']:.6f}, œÉ={threshold_stats['std']:.6f}")
print(f"Threshold (Œº+3œÉ): {threshold_stats['threshold']:.6f}")

joblib.dump(threshold_stats, "threshold_stats.pkl")  # üíæ Artefact 6/6

# ‚îÄ‚îÄ‚îÄ 7. Export ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
autoencoder.save("model_unsupervised.keras")  # üíæ Artefact 2/6
print("‚úì model_unsupervised.keras sauvegard√©")
```

---

## 3. Compatibilit√© Pipeline : Entra√Ænement ‚Üî Production

### Ordre de Traitement (CRITIQUE)

L'ordre dans `FeaturePipeline.transform()` (production) **doit √™tre identique** √† l'entra√Ænement :

| √âtape | Entra√Ænement (Colab) | Production (`feature_pipeline.py`) |
|-------|----------------------|------------------------------------|
| 1 | `DataValidator` (implicite via `dropna`, `replace`) | `DataValidator.validate()` |
| 2 | `selector.transform(X)` | `self.feature_selector.transform(X)` |
| 3 | `scaler.transform(X)` | `self.scaler.transform(X)` |

> ‚ö†Ô∏è Si vous inversez l'ordre scaler/selector entre entra√Ænement et production, les pr√©dictions seront **compl√®tement invalides** sans aucune erreur visible.

### Checklist Avant D√©ploiement

- [ ] L'`encoder.classes_` contient toutes les classes attendues (BENIGN + types d'attaques)
- [ ] Le `scaler` a √©t√© fitt√© sur les donn√©es **apr√®s** `feature_selector.transform()`
- [ ] Le `threshold_stats.pkl` a √©t√© calcul√© sur le trafic BENIGN du set de **validation** (pas train)
- [ ] Les 6 fichiers sont nomm√©s **exactement** comme dans le tableau ci-dessus
- [ ] Les fichiers sont d√©pos√©s dans `ai/artifacts/` (pas dans un sous-dossier)

---

## 4. D√©ploiement des Artefacts

```bash
# Copier depuis Colab/Jupyter vers le projet
cp model_supervised.keras  /chemin/Network-Defense-System/ai/artifacts/
cp model_unsupervised.keras /chemin/Network-Defense-System/ai/artifacts/
cp scaler.pkl              /chemin/Network-Defense-System/ai/artifacts/
cp encoder.pkl             /chemin/Network-Defense-System/ai/artifacts/
cp feature_selector.pkl    /chemin/Network-Defense-System/ai/artifacts/
cp threshold_stats.pkl     /chemin/Network-Defense-System/ai/artifacts/
```

### V√©rification

```bash
# Via Docker (les artefacts sont bind-mount√©s via docker-compose.yml)
docker compose restart backend

# V√©rifier le chargement
curl http://localhost:8000/api/models/status
```

R√©ponse attendue :
```json
{
  "is_ready": true,
  "artifacts": {
    "supervised_model": {"loaded": true, "exists": true},
    "unsupervised_model": {"loaded": true, "exists": true},
    "scaler": {"loaded": true, "exists": true},
    "encoder": {"loaded": true, "exists": true},
    "feature_selector": {"loaded": true, "exists": true}
  },
  "missing": []
}
```

> **Note** : Si les artefacts sont absents, le backend d√©marre quand m√™me en **mode d√©grad√©** (pas d'inf√©rence IA). Le log affichera : `‚ö† Artifacts AI non disponibles. Le service fonctionnera sans AI.`
